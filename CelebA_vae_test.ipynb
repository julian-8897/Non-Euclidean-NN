{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import  vae_EUCL, vae_HYP, ResNetvae_EUCL\n",
    "import geoopt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from hypmath import poincareball\n",
    "from hypmath import metrics\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "#Disable Debugging APIs\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "\n",
    "#checkpointing\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
    "    f_path = checkpoint_dir / 'checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir / 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "                        transforms.Resize(image_size),\n",
    "                        transforms.CenterCrop(image_size),\n",
    "                        transforms.ToTensor(),\n",
    "                        #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ])\n",
    "\n",
    "\n",
    "trainset = datasets.ImageFolder('data', transform=transform)\n",
    "num_data = list(range(0, 64))\n",
    "trainset_1 = torch.utils.data.Subset(trainset, num_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset_1, batch_size=64, \n",
    "                                         num_workers=1, shuffle=True, pin_memory=True)\n",
    "# dataiter = iter(trainloader)\n",
    "# images, _ = dataiter.next()\n",
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): VariationalEncoder(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "    (linear3): Linear(in_features=2048, out_features=100, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(1024, 4, 4))\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (relu): ReLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "    (d1): Linear(in_features=100, out_features=16384, bias=True)\n",
      "    (up1): UpsamplingNearest2d(scale_factor=1.0, mode=nearest)\n",
      "    (pd1): ReplicationPad2d((1, 1, 1, 1))\n",
      "    (d2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (up2): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
      "    (pd2): ReplicationPad2d((1, 1, 1, 1))\n",
      "    (d3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (up3): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
      "    (pd3): ReplicationPad2d((1, 1, 1, 1))\n",
      "    (d4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (up4): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
      "    (pd4): ReplicationPad2d((1, 1, 1, 1))\n",
      "    (d5): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (up5): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
      "    (pd5): ReplicationPad2d((1, 1, 1, 1))\n",
      "    (d6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = vae_EUCL.VariationalAutoencoder(nc=3, ndf=64, ngf=64, latent_dims=100, device=device)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = geoopt.optim.RiemannianAdam(model.parameters(), lr=learning_rate)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "\n",
    "reconstruction_function = nn.MSELoss(reduction='sum')\n",
    "#reconstruction_function.size_average = False\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "def train_epoch(vae, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for x, _ in dataloader: \n",
    "        # Move tensor to the proper device\n",
    "        x = x.to(device)\n",
    "        for param in vae.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        recon_x, mu, logvar = vae(x)\n",
    "        # Evaluate loss\n",
    "        #loss = ((x - x_hat)**2).sum() + (vae.encoder.kl)\n",
    "        #loss = BCE + (vae.encoder.kl)\n",
    "        loss = loss_function(recon_x, x, mu, logvar)\n",
    "\n",
    "        # Backward pass\n",
    "        #optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        print('\\t partial train loss (single batch): %f' % (loss.item()))\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(vae, dataloader):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    vae.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for x, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            # x = x.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = vae.encoder(x)\n",
    "            # Decode data\n",
    "            x_hat = vae(x)\n",
    "            print(x.shape)\n",
    "            loss = ((x - x_hat)**2).sum() \n",
    "            #+ vae.encoder.kl\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_outputs(encoder,decoder,n):\n",
    "    plt.figure(figsize=(10,4.5))\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = trainset_1[i][0].unsqueeze(0)\n",
    "      #img = next(iter(trainloader))\n",
    "      img = img.to(device)\n",
    "      encoder.eval()\n",
    "      decoder.eval()\n",
    "      with torch.no_grad():\n",
    "        z, _ , _ = encoder(img)\n",
    "        rec_img  = decoder(z)\n",
    "      plt.imshow(img.cpu().squeeze().permute(1, 2, 0).numpy())\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow((rec_img.cpu().squeeze().permute(1, 2, 0).numpy()))  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss (single batch): 49016.996094\n",
      "\t partial train loss (single batch): 55587.132812\n",
      "\t partial train loss (single batch): 52094.410156\n",
      "\t partial train loss (single batch): 50637.449219\n",
      "\t partial train loss (single batch): 51021.285156\n",
      "\t partial train loss (single batch): 46433.632812\n",
      "\t partial train loss (single batch): 45778.242188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_844/3336802324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m    \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n EPOCH {}/{} \\t train loss {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m    \u001b[0mplot_ae_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_844/1429385833.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(vae, dataloader, optimizer)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Print batch loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t partial train loss (single batch): %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   train_loss = train_epoch(model, trainloader, optim)\n",
    "   print('\\n EPOCH {}/{} \\t train loss {:.3f}'.format(epoch + 1, num_epochs,train_loss))\n",
    "   plot_ae_outputs(model.encoder, model.decoder,n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_samples = []\n",
    "# for sample in tqdm(trainset_1):\n",
    "#     img = sample[0].unsqueeze(0)\n",
    "#     label = sample[1]\n",
    "#     # Encode image\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         encoded_img  = model.encoder(img)\n",
    "#     # Append to list\n",
    "#     encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "#     encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "#     encoded_sample['label'] = label\n",
    "#     encoded_samples.append(encoded_sample)\n",
    "    \n",
    "# encoded_samples = pd.DataFrame(encoded_samples)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.metrics import davies_bouldin_score\n",
    "# from sklearn.metrics import calinski_harabasz_score\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# import plotly.express as px\n",
    "\n",
    "# #Davies-Bouldin Index\n",
    "# db_index = davies_bouldin_score(encoded_samples, encoded_samples.label)\n",
    "# print(db_index)\n",
    "\n",
    "# #Calinski-Harabasz Index\n",
    "# ch_score = calinski_harabasz_score(encoded_samples, encoded_samples.label)\n",
    "# print(ch_score)\n",
    "\n",
    "# # #Silhouette Coefficient\n",
    "# # s_coeff = silhouette_score(encoded_samples, encoded_samples.label,  metric=metrics.PoincareDistance)\n",
    "# # print(s_coeff)\n",
    "\n",
    "# px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2)\n",
    "# tsne_results = tsne.fit_transform(encoded_samples.drop(['label'],axis=1))\n",
    "\n",
    "# fig = px.scatter(tsne_results, x=0, y=1, color=encoded_samples.label.astype(str),labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n",
    "\n",
    "# # #Davies-Bouldin Index\n",
    "# # db_index = davies_bouldin_score(tsne_results, encoded_samples.label)\n",
    "# # print(db_index)\n",
    "\n",
    "# # #Calinski-Harabasz Index\n",
    "# # ch_score = calinski_harabasz_score(tsne_results, encoded_samples.label)\n",
    "# # print(ch_score)\n",
    "\n",
    "# # #Silhouette Coefficient\n",
    "# # s_coeff = silhouette_score(tsne_results, encoded_samples.label)\n",
    "# # print(s_coeff)\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
