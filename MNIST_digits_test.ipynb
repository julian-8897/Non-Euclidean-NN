{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e00c480ae7e3d5e7171f38ea6fedffbe731b8808f4aa360dec46acf6f1daf018"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import feed_forward\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)), \r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "n_inputs = 784\r\n",
    "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model = feed_forward.HypFF(784, 512, 256, 10)\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HypFF(\n",
       "  (fc1): MobLinear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc2): MobLinear(\n",
       "    in_features=512, out_features=256, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc3): MobLinear(\n",
       "    in_features=256, out_features=10, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HypFF(\n",
      "  (fc1): MobLinear(\n",
      "    in_features=784, out_features=512, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc2): MobLinear(\n",
      "    in_features=512, out_features=256, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc3): MobLinear(\n",
      "    in_features=256, out_features=10, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#learning_rate = 64e-1 #learning rate for logSigmoid activation function\r\n",
    "learning_rate = 2e-1 #learning rate for ReLU activation function\r\n",
    "#learning_rate = 2e-1 #current learning rate for model without activation functions\r\n",
    "momentum = 0.9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "#criterion = nn.NLLLoss()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images, labels = images.to(device), labels.to(device)\r\n",
    "images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "print(images)\r\n",
    "\r\n",
    "out = model(images) #output\r\n",
    "print(out)\r\n",
    "loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151],\n",
      "        [-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151],\n",
      "        [-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151],\n",
      "        ...,\n",
      "        [-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151],\n",
      "        [-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151],\n",
      "        [-0.0151, -0.0151, -0.0151,  ..., -0.0151, -0.0151, -0.0151]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0325,  0.0234, -0.0345,  ..., -0.0298, -0.0048, -0.0049],\n",
      "        [ 0.0259,  0.0088, -0.0117,  ..., -0.0138, -0.0103, -0.0099],\n",
      "        [ 0.0448,  0.0261, -0.0267,  ..., -0.0241, -0.0211, -0.0146],\n",
      "        ...,\n",
      "        [ 0.0468,  0.0257, -0.0009,  ..., -0.0244, -0.0203, -0.0105],\n",
      "        [ 0.0243,  0.0301, -0.0362,  ..., -0.0393, -0.0190, -0.0124],\n",
      "        [ 0.0540,  0.0296, -0.0146,  ..., -0.0280, -0.0193, -0.0156]],\n",
      "       device='cuda:0', grad_fn=<SWhereBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "\r\n",
    "time0 = time()\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "    \r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)\r\n",
    "        \r\n",
    "        #backpropagation\r\n",
    "        loss.backward()\r\n",
    "        \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "    else:\r\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 - Training loss: 1.6956563652571985\n",
      "Epoch 1 - Training loss: 1.4856361878120292\n",
      "Epoch 2 - Training loss: 1.461784232470949\n",
      "Epoch 3 - Training loss: 1.4509121735217207\n",
      "Epoch 4 - Training loss: 1.443927731554387\n",
      "Epoch 5 - Training loss: 1.4390269447181185\n",
      "Epoch 6 - Training loss: 1.4359774367283966\n",
      "Epoch 7 - Training loss: 1.4335576780771806\n",
      "Epoch 8 - Training loss: 1.4318013767064628\n",
      "Epoch 9 - Training loss: 1.4299342743421004\n",
      "\n",
      "Training Time (in minutes) = 2.891225612163544\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "images, labels = next(iter(valloader))\r\n",
    "#images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "img = 0.0357*images[0].view(1, 784)\r\n",
    "img_gpu = img.to(device)\r\n",
    "with torch.no_grad():\r\n",
    "    out = model(img_gpu)\r\n",
    "\r\n",
    "ps = out.cpu()\r\n",
    "print(ps)\r\n",
    "probab = list(ps.numpy()[0])\r\n",
    "print(probab)\r\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0877, -0.1474, -0.1010, -0.1111, -0.0761,  0.9456, -0.0774, -0.1089,\n",
      "         -0.1248, -0.0812]])\n",
      "[-0.08767295, -0.14743598, -0.10104416, -0.111054465, -0.076088525, 0.94563955, -0.07739172, -0.10891745, -0.12478899, -0.08124332]\n",
      "Predicted Digit = 5\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in valloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    img = 0.0357*images[i].view(1, 784)\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    \r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "\r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9836\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy was around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it back to the hyperbolic space), we see an increase in the model accuracy to about 97-98 percent.\r\n",
    "3. To account for the correct class probabilities , linear layer was used as the output layer instead, together with the crossentropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}