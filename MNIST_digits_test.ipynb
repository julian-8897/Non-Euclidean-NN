{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from models import ff_eucl, ff_hyp\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print('Using {}'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                #transforms.Normalize((0.1307,), (0.3081,)), \r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "train_set = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "test_set = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "\r\n",
    "size = len(train_set)\r\n",
    "print(size)\r\n",
    "\r\n",
    "train_data, val_data = torch.utils.data.random_split(train_set, [int(size-size*0.2), int(size*0.2)])\r\n",
    "\r\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=512, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=512, shuffle= True)\r\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=512, shuffle= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "ball = geoopt.PoincareBall()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images = images.view(images.shape[0], -1)\r\n",
    "grid = torchvision.utils.make_grid(images)\r\n",
    "tb = SummaryWriter()\r\n",
    "tb.add_image(\"images\", grid)\r\n",
    "tb.add_graph(model, images)\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EuclFF(\n",
       "  (act_fn): ReLU()\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "\r\n",
    "learning_rate = 4e-1 #learning rate for ReLU activation function\r\n",
    "#learning_rate = 2e-1 #current learning rate for model without activation functions\r\n",
    "momentum = 0.9\r\n",
    "weight_decay = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "# #criterion = nn.NLLLoss()\r\n",
    "# images, labels = next(iter(trainloader))\r\n",
    "# images, labels = images.to(device), labels.to(device)\r\n",
    "# images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "# print(images)\r\n",
    "\r\n",
    "# out = model(images) #output\r\n",
    "# print(out)\r\n",
    "# loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "#optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def get_num_correct(preds, labels):\r\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "\r\n",
    "# time0 = time()\r\n",
    "epochs = 10\r\n",
    "for epoch in range(epochs):\r\n",
    "    model.train()\r\n",
    "    train_loss = 0\r\n",
    "    total_correct = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        train_loss += loss.item()\r\n",
    "        total_correct += get_num_correct(output, labels)\r\n",
    "        #backpropagation\r\n",
    "        loss.backward()      \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()  \r\n",
    "\r\n",
    "    val_loss = 0\r\n",
    "    val_correct = 0\r\n",
    "    model.eval()\r\n",
    "    for  images, labels in valloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        val_loss += loss.item()\r\n",
    "        val_correct += get_num_correct(output, labels)\r\n",
    "        \r\n",
    "    tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "    tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "    tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "    tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "\r\n",
    "    print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "    \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "    \r\n",
    "tb.close()\r\n",
    "#     else:\r\n",
    "#         print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "# print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 0 training loss: 56.48072245717049 validation loss: 3.85497884452343 training accuracy: 0.8299583333333334 validation accuracy: 0.9510833333333333\n",
      "epoch: 1 training loss: 12.291927568614483 validation loss: 2.842190206050873 training accuracy: 0.959625 validation accuracy: 0.9630833333333333\n",
      "epoch: 2 training loss: 8.931119568645954 validation loss: 2.360059879720211 training accuracy: 0.9707708333333334 validation accuracy: 0.9693333333333334\n",
      "epoch: 3 training loss: 6.553195860236883 validation loss: 2.008724622428417 training accuracy: 0.978375 validation accuracy: 0.9739166666666667\n",
      "epoch: 4 training loss: 5.565003030002117 validation loss: 1.9605770744383335 training accuracy: 0.9822291666666667 validation accuracy: 0.9736666666666667\n",
      "epoch: 5 training loss: 5.418654039502144 validation loss: 2.51988335698843 training accuracy: 0.9816666666666667 validation accuracy: 0.9680833333333333\n",
      "epoch: 6 training loss: 5.132919965311885 validation loss: 1.9558661617338657 training accuracy: 0.9827708333333334 validation accuracy: 0.9746666666666667\n",
      "epoch: 7 training loss: 4.50006503239274 validation loss: 1.8893852941691875 training accuracy: 0.9848958333333333 validation accuracy: 0.9756666666666667\n",
      "epoch: 8 training loss: 4.296391407027841 validation loss: 2.026944074779749 training accuracy: 0.9854375 validation accuracy: 0.9745833333333334\n",
      "epoch: 9 training loss: 4.039751902222633 validation loss: 2.089199163019657 training accuracy: 0.986375 validation accuracy: 0.9725\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# epochs = 10\r\n",
    "# for epoch in range(epochs):\r\n",
    "#     model.eval()\r\n",
    "#     val_loss = 0\r\n",
    "#     correct = 0\r\n",
    "    \r\n",
    "#     with torch.no_grad():\r\n",
    "#         for images, labels in valloader:\r\n",
    "#             images, labels = images.to(device), labels.to(device)\r\n",
    "#             # Flatten MNIST images into a 784 long vector\r\n",
    "#             images = images.view(images.shape[0], -1)\r\n",
    "#             #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "#             output = model(images)\r\n",
    "#             loss = criterion(output, labels)  \r\n",
    "#             val_loss += loss.item()\r\n",
    "#             correct += get_num_correct(output, labels)\r\n",
    "\r\n",
    "#         tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "#         tb.add_scalar(\"Validation Accuracy\", correct/len(val_data), epoch)\r\n",
    "#         print(\"epoch:\", epoch, \"total correct:\", correct, \"validation loss:\",val_loss)\r\n",
    "#         print(\"Validation Accuracy\", correct/len(val_data))\r\n",
    "# tb.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# images, labels = next(iter(testloader))\r\n",
    "# #images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "# img = images[0].view(1, 784)\r\n",
    "# #img = ball.projx(images[0].view(1, 784))\r\n",
    "# #img_gpu = img.to(device)\r\n",
    "# with torch.no_grad():\r\n",
    "#     out = model(img)\r\n",
    "\r\n",
    "# ps = out.cpu()\r\n",
    "# print(ps)\r\n",
    "# probab = list(ps.numpy()[0])\r\n",
    "# print(probab)\r\n",
    "# print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Model Prediction and Model Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in testloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    img = images[i].view(1, 784)\r\n",
    "    #img = ball.projx(images[i].view(1, 784))\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "    \r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9716\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy was around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it back to the hyperbolic space), we see an increase in the model accuracy to about 97-98 percent.\r\n",
    "3. To account for the correct class probabilities , linear layer was used as the output layer instead, together with the crossentropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}