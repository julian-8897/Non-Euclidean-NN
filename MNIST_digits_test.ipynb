{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from models import ff_eucl, ff_hyp\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print('Using {}'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                #transforms.Normalize((0.1307,), (0.3081,)), \r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "train_set = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "test_set = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "\r\n",
    "size = len(train_set)\r\n",
    "print(size)\r\n",
    "\r\n",
    "train_data, val_data = torch.utils.data.random_split(train_set, [int(size-size*0.2), int(size*0.2)])\r\n",
    "\r\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle= True)\r\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=512, shuffle= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "# ball = geoopt.PoincareBall()\r\n",
    "# images, labels = next(iter(trainloader))\r\n",
    "# images = images.view(images.shape[0], -1)\r\n",
    "# grid = torchvision.utils.make_grid(images)\r\n",
    "# tb = SummaryWriter()\r\n",
    "# tb.add_image(\"images\", grid)\r\n",
    "# tb.add_graph(model, images)\r\n",
    "# model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "\r\n",
    "# learning_rate = 4e-1 #learning rate for ReLU activation function\r\n",
    "# #learning_rate = 2e-1 #current learning rate for model without activation functions\r\n",
    "# momentum = 0.9\r\n",
    "# weight_decay = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "from itertools import product\r\n",
    "parameters = dict(\r\n",
    "    lr = [0.1, 0.01],\r\n",
    "    batch_size = [64,128],\r\n",
    "    shuffle = [True, False]\r\n",
    ")\r\n",
    "\r\n",
    "param_values = [v for v in parameters.values()]\r\n",
    "print(param_values)\r\n",
    "\r\n",
    "for lr,batch_size, shuffle in product(*param_values):\r\n",
    "    print(lr, batch_size, shuffle)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.1, 0.01], [64, 128], [True, False]]\n",
      "0.1 64 True\n",
      "0.1 64 False\n",
      "0.1 128 True\n",
      "0.1 128 False\n",
      "0.01 64 True\n",
      "0.01 64 False\n",
      "0.01 128 True\n",
      "0.01 128 False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "#criterion = nn.CrossEntropyLoss()\r\n",
    "# #criterion = nn.NLLLoss()\r\n",
    "# images, labels = next(iter(trainloader))\r\n",
    "# images, labels = images.to(device), labels.to(device)\r\n",
    "# images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "# print(images)\r\n",
    "\r\n",
    "# out = model(images) #output\r\n",
    "# print(out)\r\n",
    "# loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "#optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)\r\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "### Single prediction function\r\n",
    "def get_num_correct(preds, labels):\r\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\r\n",
    "\r\n",
    "### Training function\r\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\r\n",
    "    model.train()\r\n",
    "    train_loss = 0\r\n",
    "    total_correct = 0\r\n",
    "    for images, labels in dataloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        train_loss += loss.item()\r\n",
    "        total_correct += get_num_correct(output, labels)\r\n",
    "        #backpropagation\r\n",
    "        loss.backward()      \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()  \r\n",
    "\r\n",
    "    return train_loss, total_correct\r\n",
    "\r\n",
    "### Validation function\r\n",
    "def val_epoch(model, dataloader, criterion):\r\n",
    "    model.eval()\r\n",
    "    val_loss = 0\r\n",
    "    val_correct = 0\r\n",
    "    for  images, labels in dataloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        val_loss += loss.item()\r\n",
    "        val_correct += get_num_correct(output, labels)\r\n",
    "    \r\n",
    "    return val_loss, val_correct\r\n",
    "\r\n",
    "\r\n",
    "### Hyperparameter tuning function\r\n",
    "def hparams_tune(epochs):\r\n",
    "    for run_id, (lr,batch_size, shuffle) in enumerate(product(*param_values)):\r\n",
    "        print(\"run id:\", run_id + 1)\r\n",
    "        model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "        trainloader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, shuffle = shuffle)\r\n",
    "        valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle= shuffle)\r\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "        criterion = torch.nn.CrossEntropyLoss()\r\n",
    "        comment = f' batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\r\n",
    "        tb = SummaryWriter(comment=comment)\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            train_loss, total_correct = train_epoch(model, trainloader, optimizer)\r\n",
    "            val_loss, val_correct = val_epoch(model, valloader)\r\n",
    "                \r\n",
    "            tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "            tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "            tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "            tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "\r\n",
    "            print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "            \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "        \r\n",
    "        tb.add_hparams(\r\n",
    "                {\"lr\": lr, \"bsize\": batch_size, \"shuffle\":shuffle},\r\n",
    "                {\r\n",
    "                    \"training accuracy\": total_correct/ len(train_data),\r\n",
    "                    \"validation accuracy\": val_correct/ len(val_data),\r\n",
    "                    \"training loss\": train_loss,\r\n",
    "                    \"validation loss\": val_loss,\r\n",
    "                },\r\n",
    "            )\r\n",
    "    tb.close()\r\n",
    "\r\n",
    "    return None\r\n",
    "\r\n",
    "### Model evaluation\r\n",
    "def model_eval(model, epochs, trainloader, valloader, optimizer, criterion, tuning=True,):\r\n",
    "    if tuning is False:\r\n",
    "        for epoch in range(epochs):\r\n",
    "            train_loss, total_correct = train_epoch(model, trainloader, optimizer, criterion) \r\n",
    "            val_loss, val_correct = val_epoch(model, valloader, criterion)\r\n",
    "            print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "            \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "    \r\n",
    "    else:\r\n",
    "        hparams_tune(epochs)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\r\n",
    "criterion = torch.nn.CrossEntropyLoss()\r\n",
    "model_eval(model, 10, trainloader, valloader, optimizer, criterion, tuning=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 0 training loss: 391.2765506133437 validation loss: 47.783599846065044 training accuracy: 0.8581875 validation accuracy: 0.9264166666666667\n",
      "epoch: 1 training loss: 144.65838014148176 validation loss: 33.09455743059516 training accuracy: 0.9434375 validation accuracy: 0.9495\n",
      "epoch: 2 training loss: 97.8371591316536 validation loss: 26.386380705982447 training accuracy: 0.9610208333333333 validation accuracy: 0.95825\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/3403425656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/340130651.py\u001b[0m in \u001b[0;36mmodel_eval\u001b[1;34m(model, epochs, trainloader, valloader, optimizer, criterion, tuning)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtuning\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/340130651.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Flatten MNIST images into a 784 long vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for run_id, (lr,batch_size, shuffle) in enumerate(product(*param_values)):\r\n",
    "#     print(\"run id:\", run_id + 1)\r\n",
    "#     model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "#     trainloader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, shuffle = shuffle)\r\n",
    "#     valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle= shuffle)\r\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\r\n",
    "#     comment = f' batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\r\n",
    "#     tb = SummaryWriter(comment=comment)\r\n",
    "# # time0 = time()\r\n",
    "#     epochs = 10\r\n",
    "#     for epoch in range(epochs):\r\n",
    "#     #     model.train()\r\n",
    "#     #     train_loss = 0\r\n",
    "#     #     total_correct = 0\r\n",
    "#     #     for images, labels in trainloader:\r\n",
    "#     #         images, labels = images.to(device), labels.to(device)\r\n",
    "#     #         # Flatten MNIST images into a 784 long vector\r\n",
    "#     #         images = images.view(images.shape[0], -1)\r\n",
    "#     #         #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "#     #         # Training pass\r\n",
    "#     #         optimizer.zero_grad()\r\n",
    "#     #         output = model(images)\r\n",
    "#     #         loss = criterion(output, labels)  \r\n",
    "#     #         train_loss += loss.item()\r\n",
    "#     #         total_correct += get_num_correct(output, labels)\r\n",
    "#     #         #backpropagation\r\n",
    "#     #         loss.backward()      \r\n",
    "#     #         #Weight optimization\r\n",
    "#     #         optimizer.step()  \r\n",
    "#         train_loss, total_correct = train_epoch(model, trainloader, optimizer)\r\n",
    "#         val_loss, val_correct = val_epoch(model, valloader)\r\n",
    "#         # val_loss = 0\r\n",
    "#         # val_correct = 0\r\n",
    "#         # model.eval()\r\n",
    "#         # for  images, labels in valloader:\r\n",
    "#         #     images, labels = images.to(device), labels.to(device)\r\n",
    "#         #     # Flatten MNIST images into a 784 long vector\r\n",
    "#         #     images = images.view(images.shape[0], -1)\r\n",
    "#         #     #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "#         #     output = model(images)\r\n",
    "#         #     loss = criterion(output, labels)  \r\n",
    "#         #     val_loss += loss.item()\r\n",
    "#         #     val_correct += get_num_correct(output, labels)\r\n",
    "            \r\n",
    "#         tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "#         tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "#         tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "#         tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "\r\n",
    "#         print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "#         \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "    \r\n",
    "#     tb.add_hparams(\r\n",
    "#             {\"lr\": lr, \"bsize\": batch_size, \"shuffle\":shuffle},\r\n",
    "#             {\r\n",
    "#                 \"training accuracy\": total_correct/ len(train_data),\r\n",
    "#                 \"validation accuracy\": val_correct/ len(val_data),\r\n",
    "#                 \"training loss\": train_loss,\r\n",
    "#                 \"validation loss\": val_loss,\r\n",
    "#             },\r\n",
    "#         )\r\n",
    "# tb.close()\r\n",
    "# #     else:\r\n",
    "# #         print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "# # print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run id: 1\n",
      "epoch: 0 training loss: 392.1312243938446 validation loss: 45.524742260575294 training accuracy: 0.8544166666666667 validation accuracy: 0.9294166666666667\n",
      "epoch: 1 training loss: 148.6686474904418 validation loss: 30.336301969364285 training accuracy: 0.9421041666666666 validation accuracy: 0.9530833333333333\n",
      "epoch: 2 training loss: 100.84368281438947 validation loss: 24.182748220860958 training accuracy: 0.9600416666666667 validation accuracy: 0.963\n",
      "epoch: 3 training loss: 74.7791788065806 validation loss: 19.343951125629246 training accuracy: 0.9703958333333333 validation accuracy: 0.97025\n",
      "epoch: 4 training loss: 57.605175531469285 validation loss: 17.36317464709282 training accuracy: 0.9774166666666667 validation accuracy: 0.9719166666666667\n",
      "epoch: 5 training loss: 45.906268164515495 validation loss: 17.34404821647331 training accuracy: 0.9821458333333334 validation accuracy: 0.9715833333333334\n",
      "epoch: 6 training loss: 37.498522373382 validation loss: 15.201203659642488 training accuracy: 0.9850833333333333 validation accuracy: 0.9744166666666667\n",
      "epoch: 7 training loss: 29.955470580607653 validation loss: 15.090021434705704 training accuracy: 0.9889166666666667 validation accuracy: 0.9754166666666667\n",
      "epoch: 8 training loss: 24.52989673463162 validation loss: 14.420503607252613 training accuracy: 0.9911875 validation accuracy: 0.9753333333333334\n",
      "epoch: 9 training loss: 20.070684514008462 validation loss: 14.320263517089188 training accuracy: 0.9929166666666667 validation accuracy: 0.97625\n",
      "run id: 2\n",
      "epoch: 0 training loss: 395.7362251803279 validation loss: 46.54762249439955 training accuracy: 0.8505416666666666 validation accuracy: 0.9269166666666667\n",
      "epoch: 1 training loss: 150.2083881907165 validation loss: 30.93691621348262 training accuracy: 0.9406458333333333 validation accuracy: 0.95025\n",
      "epoch: 2 training loss: 101.41721570119262 validation loss: 24.177383905276656 training accuracy: 0.9604791666666667 validation accuracy: 0.9613333333333334\n",
      "epoch: 3 training loss: 75.51036809198558 validation loss: 20.474730048328638 training accuracy: 0.9712708333333333 validation accuracy: 0.96725\n",
      "epoch: 4 training loss: 58.66315545607358 validation loss: 18.40545068308711 training accuracy: 0.9775625 validation accuracy: 0.9699166666666666\n",
      "epoch: 5 training loss: 46.61169364838861 validation loss: 17.2190150488168 training accuracy: 0.9826666666666667 validation accuracy: 0.97175\n",
      "epoch: 6 training loss: 37.299495566869155 validation loss: 16.431855898350477 training accuracy: 0.98675 validation accuracy: 0.9729166666666667\n",
      "epoch: 7 training loss: 29.9322210655082 validation loss: 16.002598900347948 training accuracy: 0.9897916666666666 validation accuracy: 0.9735\n",
      "epoch: 8 training loss: 23.98746394540649 validation loss: 15.732723208609968 training accuracy: 0.9924166666666666 validation accuracy: 0.9748333333333333\n",
      "epoch: 9 training loss: 19.157376851071604 validation loss: 15.449541814625263 training accuracy: 0.9945208333333333 validation accuracy: 0.9755833333333334\n",
      "run id: 3\n",
      "epoch: 0 training loss: 278.85215696692467 validation loss: 30.589119017124176 training accuracy: 0.803125 validation accuracy: 0.90575\n",
      "epoch: 1 training loss: 106.36561222374439 validation loss: 23.431330516934395 training accuracy: 0.9170208333333333 validation accuracy: 0.92775\n",
      "epoch: 2 training loss: 79.59704450517893 validation loss: 18.055617339909077 training accuracy: 0.9385833333333333 validation accuracy: 0.9435833333333333\n",
      "epoch: 3 training loss: 62.78241118788719 validation loss: 15.049262546002865 training accuracy: 0.9511666666666667 validation accuracy: 0.9545833333333333\n",
      "epoch: 4 training loss: 51.39439858868718 validation loss: 13.08991527557373 training accuracy: 0.9596666666666667 validation accuracy: 0.9585\n",
      "epoch: 5 training loss: 42.65958511084318 validation loss: 11.444974198937416 training accuracy: 0.9668333333333333 validation accuracy: 0.96525\n",
      "epoch: 6 training loss: 36.299462385475636 validation loss: 11.062482558190823 training accuracy: 0.9716666666666667 validation accuracy: 0.9656666666666667\n",
      "epoch: 7 training loss: 30.954201109707355 validation loss: 9.636698964983225 training accuracy: 0.9761666666666666 validation accuracy: 0.9703333333333334\n",
      "epoch: 8 training loss: 26.753229529596865 validation loss: 9.088328244164586 training accuracy: 0.9798958333333333 validation accuracy: 0.9721666666666666\n",
      "epoch: 9 training loss: 23.274786132387817 validation loss: 8.819008886814117 training accuracy: 0.9829375 validation accuracy: 0.9715\n",
      "run id: 4\n",
      "epoch: 0 training loss: 279.91763189435005 validation loss: 30.42346103489399 training accuracy: 0.8097708333333333 validation accuracy: 0.9058333333333334\n",
      "epoch: 1 training loss: 107.66348516196012 validation loss: 22.82789544761181 training accuracy: 0.9166875 validation accuracy: 0.9288333333333333\n",
      "epoch: 2 training loss: 81.36632722616196 validation loss: 18.290640726685524 training accuracy: 0.9368541666666667 validation accuracy: 0.9413333333333334\n",
      "epoch: 3 training loss: 64.16020460426807 validation loss: 15.351017199456692 training accuracy: 0.9502916666666666 validation accuracy: 0.95125\n",
      "epoch: 4 training loss: 52.10059421509504 validation loss: 13.25391897186637 training accuracy: 0.9593125 validation accuracy: 0.9585\n",
      "epoch: 5 training loss: 43.23383803293109 validation loss: 11.762958150357008 training accuracy: 0.9668541666666667 validation accuracy: 0.9631666666666666\n",
      "epoch: 6 training loss: 36.55075423978269 validation loss: 10.710682041943073 training accuracy: 0.9724791666666667 validation accuracy: 0.96725\n",
      "epoch: 7 training loss: 31.312594952061772 validation loss: 9.92673821374774 training accuracy: 0.9767291666666666 validation accuracy: 0.9693333333333334\n",
      "epoch: 8 training loss: 27.104666314087808 validation loss: 9.351745216175914 training accuracy: 0.9800833333333333 validation accuracy: 0.9705\n",
      "epoch: 9 training loss: 23.59044230170548 validation loss: 8.898410132154822 training accuracy: 0.9827708333333334 validation accuracy: 0.9713333333333334\n",
      "run id: 5\n",
      "epoch: 0 training loss: 1358.8078846931458 validation loss: 178.76608937978745 training accuracy: 0.5965416666666666 validation accuracy: 0.7941666666666667\n",
      "epoch: 1 training loss: 472.41868671774864 validation loss: 88.45005449652672 training accuracy: 0.844375 validation accuracy: 0.8763333333333333\n",
      "epoch: 2 training loss: 313.1273464560509 validation loss: 71.82358777523041 training accuracy: 0.8849375 validation accuracy: 0.8918333333333334\n",
      "epoch: 3 training loss: 268.2271773070097 validation loss: 63.88795866072178 training accuracy: 0.8985208333333333 validation accuracy: 0.904\n",
      "epoch: 4 training loss: 244.30545619130135 validation loss: 59.91187082976103 training accuracy: 0.9076875 validation accuracy: 0.9089166666666667\n",
      "epoch: 5 training loss: 227.1899670213461 validation loss: 55.8631826415658 training accuracy: 0.9135833333333333 validation accuracy: 0.9145\n",
      "epoch: 6 training loss: 213.84293923527002 validation loss: 52.83025299012661 training accuracy: 0.9183333333333333 validation accuracy: 0.9185\n",
      "epoch: 7 training loss: 201.7585610896349 validation loss: 50.21255250275135 training accuracy: 0.9236041666666667 validation accuracy: 0.9206666666666666\n",
      "epoch: 8 training loss: 191.27062325924635 validation loss: 47.76613298431039 training accuracy: 0.9277916666666667 validation accuracy: 0.9278333333333333\n",
      "epoch: 9 training loss: 180.95653211697936 validation loss: 45.70437599718571 training accuracy: 0.9318125 validation accuracy: 0.9295833333333333\n",
      "run id: 6\n",
      "epoch: 0 training loss: 1381.0131884813309 validation loss: 184.71999418735504 training accuracy: 0.6026041666666667 validation accuracy: 0.788\n",
      "epoch: 1 training loss: 483.0171950161457 validation loss: 90.19208681583405 training accuracy: 0.8382291666666667 validation accuracy: 0.87\n",
      "epoch: 2 training loss: 318.2864802032709 validation loss: 72.31621439754963 training accuracy: 0.8839166666666667 validation accuracy: 0.89175\n",
      "epoch: 3 training loss: 271.30660063028336 validation loss: 64.68049874901772 training accuracy: 0.8981666666666667 validation accuracy: 0.9016666666666666\n",
      "epoch: 4 training loss: 246.77441827207804 validation loss: 59.92739628255367 training accuracy: 0.9068125 validation accuracy: 0.9074166666666666\n",
      "epoch: 5 training loss: 229.73688034713268 validation loss: 56.3366337120533 training accuracy: 0.9127083333333333 validation accuracy: 0.9134166666666667\n",
      "epoch: 6 training loss: 216.01469027996063 validation loss: 53.30218934267759 training accuracy: 0.918 validation accuracy: 0.9174166666666667\n",
      "epoch: 7 training loss: 203.97991443425417 validation loss: 50.60304147750139 training accuracy: 0.9225833333333333 validation accuracy: 0.9211666666666667\n",
      "epoch: 8 training loss: 193.00307713449 validation loss: 48.116859279572964 training accuracy: 0.9266041666666667 validation accuracy: 0.9244166666666667\n",
      "epoch: 9 training loss: 182.8440112620592 validation loss: 45.81137879192829 training accuracy: 0.93075 validation accuracy: 0.9276666666666666\n",
      "run id: 7\n",
      "epoch: 0 training loss: 817.2592813968658 validation loss: 183.30691587924957 training accuracy: 0.50675 validation accuracy: 0.70175\n",
      "epoch: 1 training loss: 524.8150735497475 validation loss: 84.45753818750381 training accuracy: 0.7566666666666667 validation accuracy: 0.8080833333333334\n",
      "epoch: 2 training loss: 264.0988395214081 validation loss: 53.88132005929947 training accuracy: 0.8307083333333334 validation accuracy: 0.85075\n",
      "epoch: 3 training loss: 193.15647447109222 validation loss: 43.671344727277756 training accuracy: 0.8647083333333333 validation accuracy: 0.8749166666666667\n",
      "epoch: 4 training loss: 163.73057967424393 validation loss: 38.45923861861229 training accuracy: 0.8822916666666667 validation accuracy: 0.88725\n",
      "epoch: 5 training loss: 147.7679006755352 validation loss: 35.45550914108753 training accuracy: 0.8912083333333334 validation accuracy: 0.8963333333333333\n",
      "epoch: 6 training loss: 137.46546778082848 validation loss: 33.49730144441128 training accuracy: 0.8975416666666667 validation accuracy: 0.8996666666666666\n",
      "epoch: 7 training loss: 130.14451895654202 validation loss: 31.99679324030876 training accuracy: 0.90125 validation accuracy: 0.9019166666666667\n",
      "epoch: 8 training loss: 124.35091437399387 validation loss: 30.77287982404232 training accuracy: 0.9054166666666666 validation accuracy: 0.907\n",
      "epoch: 9 training loss: 119.57779628038406 validation loss: 29.738623157143593 training accuracy: 0.9095 validation accuracy: 0.9094166666666667\n",
      "run id: 8\n",
      "epoch: 0 training loss: 818.0721888542175 validation loss: 183.8776649236679 training accuracy: 0.5078541666666667 validation accuracy: 0.6184166666666666\n",
      "epoch: 1 training loss: 537.7519242763519 validation loss: 89.01153481006622 training accuracy: 0.71875 validation accuracy: 0.7986666666666666\n",
      "epoch: 2 training loss: 273.9130778312683 validation loss: 55.33115842938423 training accuracy: 0.8313333333333334 validation accuracy: 0.8535833333333334\n",
      "epoch: 3 training loss: 195.60159376263618 validation loss: 44.17066532373428 training accuracy: 0.8660833333333333 validation accuracy: 0.8741666666666666\n",
      "epoch: 4 training loss: 164.63625828921795 validation loss: 38.88811591267586 training accuracy: 0.8820416666666666 validation accuracy: 0.8848333333333334\n",
      "epoch: 5 training loss: 148.19121800363064 validation loss: 35.77367952466011 training accuracy: 0.89175 validation accuracy: 0.8925\n",
      "epoch: 6 training loss: 137.70348727703094 validation loss: 33.65565983951092 training accuracy: 0.8974375 validation accuracy: 0.8986666666666666\n",
      "epoch: 7 training loss: 130.1535985171795 validation loss: 32.06874652206898 training accuracy: 0.9020416666666666 validation accuracy: 0.9035\n",
      "epoch: 8 training loss: 124.24875888973475 validation loss: 30.793994829058647 training accuracy: 0.9057083333333333 validation accuracy: 0.9059166666666667\n",
      "epoch: 9 training loss: 119.35912036895752 validation loss: 29.717377960681915 training accuracy: 0.9092083333333333 validation accuracy: 0.9095833333333333\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# images, labels = next(iter(testloader))\r\n",
    "# #images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "# img = images[0].view(1, 784)\r\n",
    "# #img = ball.projx(images[0].view(1, 784))\r\n",
    "# #img_gpu = img.to(device)\r\n",
    "# with torch.no_grad():\r\n",
    "#     out = model(img)\r\n",
    "\r\n",
    "# ps = out.cpu()\r\n",
    "# print(ps)\r\n",
    "# probab = list(ps.numpy()[0])\r\n",
    "# print(probab)\r\n",
    "# print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Model Prediction and Model Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in testloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    img = images[i].view(1, 784)\r\n",
    "    #img = ball.projx(images[i].view(1, 784))\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "    \r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9136\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy was around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it back to the hyperbolic space), we see an increase in the model accuracy to about 97-98 percent.\r\n",
    "3. To account for the correct class probabilities , linear layer was used as the output layer instead, together with the crossentropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}