{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import feed_forward\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print('Using {}'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, mlc, xla, vulkan, meta, hpu device type at start of device string: CUDA",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15200/1987874605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CUDA'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'CPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, mlc, xla, vulkan, meta, hpu device type at start of device string: CUDA"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)), \r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = feed_forward.HypFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "ball = geoopt.PoincareBall()\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HypFF(\n",
       "  (act_fn): ReLU()\n",
       "  (fc1): MobLinear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc2): MobLinear(\n",
       "    in_features=512, out_features=256, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc3): MobLinear(\n",
       "    in_features=256, out_features=10, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#learning_rate = 8e-1 \r\n",
    "learning_rate = 4e-1 #learning rate for ReLU activation function\r\n",
    "#learning_rate = 2e-1 #current learning rate for model without activation functions\r\n",
    "momentum = 0.9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "#criterion = nn.NLLLoss()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images, labels = images.to(device), labels.to(device)\r\n",
    "images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "print(images)\r\n",
    "\r\n",
    "out = model(images) #output\r\n",
    "print(out)\r\n",
    "loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[-0.3200, -0.3076, -0.3126,  ..., -0.3110, -0.3221, -0.3124],\n",
      "        [-0.3202, -0.3074, -0.3125,  ..., -0.3113, -0.3223, -0.3120],\n",
      "        [-0.3202, -0.3074, -0.3126,  ..., -0.3115, -0.3221, -0.3120],\n",
      "        ...,\n",
      "        [-0.3201, -0.3073, -0.3126,  ..., -0.3111, -0.3220, -0.3125],\n",
      "        [-0.3200, -0.3071, -0.3128,  ..., -0.3111, -0.3225, -0.3121],\n",
      "        [-0.3200, -0.3074, -0.3127,  ..., -0.3112, -0.3224, -0.3122]],\n",
      "       device='cuda:0', grad_fn=<SWhereBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "time0 = time()\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        #images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "        images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        \r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)\r\n",
    "        \r\n",
    "        #backpropagation\r\n",
    "        loss.backward()\r\n",
    "        \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "    else:\r\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 - Training loss: 0.30994176081681657\n",
      "Epoch 1 - Training loss: 0.1534982708803678\n",
      "Epoch 2 - Training loss: 0.06521061056498755\n",
      "Epoch 3 - Training loss: 0.05067854955539865\n",
      "Epoch 4 - Training loss: 0.04507155040815725\n",
      "Epoch 5 - Training loss: 0.041322239216859055\n",
      "Epoch 6 - Training loss: 0.038477109902996125\n",
      "Epoch 7 - Training loss: 0.036618647844357005\n",
      "Epoch 8 - Training loss: 0.035227720231070354\n",
      "Epoch 9 - Training loss: 0.034064377642284004\n",
      "\n",
      "Training Time (in minutes) = 2.5823063254356384\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "images, labels = next(iter(valloader))\r\n",
    "#images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "#img = 0.0357*images[0].view(1, 784)\r\n",
    "img = ball.projx(images[0].view(1, 784))\r\n",
    "img_gpu = img.to(device)\r\n",
    "with torch.no_grad():\r\n",
    "    out = model(img_gpu)\r\n",
    "\r\n",
    "ps = out.cpu()\r\n",
    "print(ps)\r\n",
    "probab = list(ps.numpy()[0])\r\n",
    "print(probab)\r\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'valloader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12216/1876363490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#images, labels = images.to(device), labels.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#img = 0.0357*images[0].view(1, 784)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mball\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valloader' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in valloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    #img = 0.0357*images[i].view(1, 784)\r\n",
    "    img = ball.projx(images[i].view(1, 784))\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    \r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "\r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9743\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy was around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it back to the hyperbolic space), we see an increase in the model accuracy to about 97-98 percent.\r\n",
    "3. To account for the correct class probabilities , linear layer was used as the output layer instead, together with the crossentropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}