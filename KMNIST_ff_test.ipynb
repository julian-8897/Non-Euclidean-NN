{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MNIST experiment for FF models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ff_eucl, ff_hyp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import geoopt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.cuda.is_available()\n",
    "\n",
    "#Disable Debugging APIs\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "\n",
    "#cuDNN Autotuner\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor() \n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.KMNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\n",
    "test_set = datasets.KMNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\n",
    "\n",
    "size = len(train_set)\n",
    "print(size)\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_set, [int(size-size*0.2), int(size*0.2)])\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=6, pin_memory=True)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle= True, num_workers=6, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle= True, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    \"\"\"\n",
    "    Single Prediction function\n",
    "    \"\"\"\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Model training function\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_correct = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        # for param in model.parameters():\n",
    "        #     param.grad = None\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)  \n",
    "        train_loss += loss.item()\n",
    "        total_correct += get_num_correct(output, labels)\n",
    "        #backpropagation\n",
    "        loss.backward()      \n",
    "        #Weight optimization\n",
    "        optimizer.step()  \n",
    "\n",
    "    return train_loss/len(dataloader.dataset), total_correct\n",
    "\n",
    "### Validation function\n",
    "def val_epoch(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Model validation function\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for  images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            #images = ball.projx(images.view(images.shape[0], -1))\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)  \n",
    "            val_loss += loss.item()\n",
    "            val_correct += get_num_correct(output, labels)\n",
    "    \n",
    "    return val_loss/len(dataloader.dataset), val_correct\n",
    "\n",
    "\n",
    "def model_eval(model, epochs, trainloader, valloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function for model evaluation\n",
    "    \"\"\"\n",
    "    tb = SummaryWriter()\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    t_accuracy = []\n",
    "    v_accuracy = []\n",
    "    epoch_values = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, total_correct = train_epoch(model, trainloader, optimizer, criterion) \n",
    "        t_loss.append(train_loss)\n",
    "        t_accuracy.append(total_correct/len(train_data))\n",
    "        val_loss, val_correct = val_epoch(model, valloader, criterion)\n",
    "        v_loss.append(val_loss)\n",
    "        v_accuracy.append(val_correct/len(val_data))\n",
    "        epoch_values.append(epoch)\n",
    "        \n",
    "        tb.add_scalar(\"Training Loss\", train_loss, epoch)\n",
    "        tb.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "        tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\n",
    "        tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\n",
    "        print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\n",
    "        \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\n",
    "\n",
    "\n",
    "    return t_loss, v_loss, t_accuracy, v_accuracy, epoch_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\n",
    "model = ff_hyp.HypFF(784, 512, 256, 10, nn.ReLU())\n",
    "print(model)\n",
    "\n",
    "epochs = 10\n",
    "#Hyperparameter tuning\n",
    "#hparams_tune(epochs)\n",
    "#Model evaluation\n",
    "lr=0.01\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = torch.nn.NLLLoss()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "t_loss ,v_loss, t_accuracy, v_accuracy, epoch_values = model_eval(model, epochs, trainloader, valloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig , (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "# ax0 = fig.add_subplot(121, title=\"Loss curves\")\n",
    "# ax1 = fig.add_subplot(122, title=\"Accuracy curves\")\n",
    "ax0.set_title('Loss Curves')\n",
    "ax1.set_title('Accuracy Curves')\n",
    "ax0.plot(epoch_values, t_loss, 'bo-', label='train')\n",
    "ax0.plot(epoch_values, v_loss, 'ro-', label='val')\n",
    "ax1.plot(epoch_values, t_accuracy, 'bo-', label='train')\n",
    "ax1.plot(epoch_values, v_accuracy, 'ro-', label='val')\n",
    "\n",
    "\n",
    "ax1.yaxis.set_ticks(np.arange(0.7, 1.0, 0.02))\n",
    "ax1.set_ylim(0.7, 1.0)\n",
    "\n",
    "ax0.set_xlabel('Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax0.set_ylabel('Losses')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax0.legend()\n",
    "ax1.legend()\n",
    "\n",
    "fig.suptitle('no. of epochs = {}, lr = {}, batch size = 64'.format(epochs, lr))\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
