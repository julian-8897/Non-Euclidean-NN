{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e00c480ae7e3d5e7171f38ea6fedffbe731b8808f4aa360dec46acf6f1daf018"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import feed_forward\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "n_inputs = 784\r\n",
    "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = feed_forward.HypFF()\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HypFF(\n",
       "  (fc1): MobLinear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc2): MobLinear(\n",
       "    in_features=512, out_features=256, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc3): MobLinear(\n",
       "    in_features=256, out_features=10, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HypFF(\n",
      "  (fc1): MobLinear(\n",
      "    in_features=784, out_features=512, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc2): MobLinear(\n",
      "    in_features=512, out_features=256, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc3): MobLinear(\n",
      "    in_features=256, out_features=10, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#learning_rate = 16e-4\r\n",
    "learning_rate = 2e-1 #current learning rate for model without activation functions\r\n",
    "momentum = 0.9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images, labels = images.to(device), labels.to(device)\r\n",
    "images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "\r\n",
    "out = model(images) #output\r\n",
    "print(out)\r\n",
    "loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.3168, -0.3062, -0.3142,  ..., -0.3216, -0.3079, -0.3232],\n",
      "        [-0.3172, -0.3067, -0.3143,  ..., -0.3220, -0.3067, -0.3224],\n",
      "        [-0.3169, -0.3064, -0.3147,  ..., -0.3216, -0.3077, -0.3230],\n",
      "        ...,\n",
      "        [-0.3178, -0.3064, -0.3144,  ..., -0.3216, -0.3068, -0.3223],\n",
      "        [-0.3176, -0.3066, -0.3139,  ..., -0.3218, -0.3070, -0.3221],\n",
      "        [-0.3182, -0.3069, -0.3145,  ..., -0.3206, -0.3077, -0.3218]],\n",
      "       device='cuda:0', grad_fn=<SWhereBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "time0 = time()\r\n",
    "\r\n",
    "epochs = 15\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "    \r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)\r\n",
    "        \r\n",
    "        #backpropagation\r\n",
    "        loss.backward()\r\n",
    "        \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "    else:\r\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 - Training loss: 2.2727401014101707\n",
      "Epoch 1 - Training loss: 2.120354413986206\n",
      "Epoch 2 - Training loss: 2.0765020281581554\n",
      "Epoch 3 - Training loss: 2.059266195458881\n",
      "Epoch 4 - Training loss: 2.05212875341965\n",
      "Epoch 5 - Training loss: 2.04829455028146\n",
      "Epoch 6 - Training loss: 2.04579631756928\n",
      "Epoch 7 - Training loss: 2.0440651966353593\n",
      "Epoch 8 - Training loss: 2.0425222021038247\n",
      "Epoch 9 - Training loss: 2.0414136951252564\n",
      "Epoch 10 - Training loss: 2.0405008772672235\n",
      "Epoch 11 - Training loss: 2.039683014659558\n",
      "Epoch 12 - Training loss: 2.0390893806845454\n",
      "Epoch 13 - Training loss: 2.0385063785617636\n",
      "Epoch 14 - Training loss: 2.038094476117926\n",
      "\n",
      "Training Time (in minutes) = 4.705999251206716\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "images, labels = next(iter(valloader))\r\n",
    "#images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "img = 0.0357*images[0].view(1, 784)\r\n",
    "img_gpu = img.to(device)\r\n",
    "with torch.no_grad():\r\n",
    "    out = model(img_gpu)\r\n",
    "\r\n",
    "ps = out.cpu()\r\n",
    "probab = list(ps.numpy()[0])\r\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Digit = 6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in valloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    img = 0.0357*images[i].view(1, 784)\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    \r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "\r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.977\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy converged around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it nack to the hyperbolic space), we see an increase in the model accuracy to about 97.7 percent.\r\n",
    "3. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}