{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e00c480ae7e3d5e7171f38ea6fedffbe731b8808f4aa360dec46acf6f1daf018"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "import feed_forward\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Test data from MNIST data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "n_inputs = 784\r\n",
    "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\r\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "model = feed_forward.HypFF()\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HypFF(\n",
       "  (fc1): MobLinear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc2): MobLinear(\n",
       "    in_features=512, out_features=256, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       "  (fc3): MobLinear(\n",
       "    in_features=256, out_features=10, bias=True\n",
       "    (ball): PoincareBall manifold\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 206
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HypFF(\n",
      "  (fc1): MobLinear(\n",
      "    in_features=784, out_features=512, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc2): MobLinear(\n",
      "    in_features=512, out_features=256, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      "  (fc3): MobLinear(\n",
      "    in_features=256, out_features=10, bias=True\n",
      "    (ball): PoincareBall manifold\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "#learning_rate = 16e-4\r\n",
    "learning_rate = 2e-1\r\n",
    "momentum = 0.9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images, labels = images.to(device), labels.to(device)\r\n",
    "images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "\r\n",
    "out = model(images) #output\r\n",
    "print(out)\r\n",
    "loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0374, -0.0477, -0.0111,  ...,  0.0951,  0.0022,  0.0168],\n",
      "        [ 0.0203, -0.0545, -0.0251,  ...,  0.0947, -0.0069,  0.0272],\n",
      "        [ 0.0450, -0.0522, -0.0068,  ...,  0.0820,  0.0146,  0.0285],\n",
      "        ...,\n",
      "        [ 0.0248, -0.0553, -0.0110,  ...,  0.0848,  0.0143,  0.0189],\n",
      "        [ 0.0302, -0.0461, -0.0046,  ...,  0.0870,  0.0028,  0.0262],\n",
      "        [ 0.0347, -0.0518, -0.0040,  ...,  0.0890, -0.0174,  0.0211]],\n",
      "       device='cuda:0', grad_fn=<SWhereBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "source": [
    "\r\n",
    "time0 = time()\r\n",
    "\r\n",
    "epochs = 15\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "    \r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)\r\n",
    "        \r\n",
    "        #backpropagation\r\n",
    "        loss.backward()\r\n",
    "        \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "    else:\r\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 - Training loss: 1.834037165520555\n",
      "Epoch 1 - Training loss: 1.660454516693697\n",
      "Epoch 2 - Training loss: 1.6410405029684811\n",
      "Epoch 3 - Training loss: 1.6324574937254697\n",
      "Epoch 4 - Training loss: 1.6284777350344901\n",
      "Epoch 5 - Training loss: 1.6246196082082844\n",
      "Epoch 6 - Training loss: 1.6231947672569145\n",
      "Epoch 7 - Training loss: 1.6194857439752353\n",
      "Epoch 8 - Training loss: 1.6168246087381395\n",
      "Epoch 9 - Training loss: 1.6109006273544442\n",
      "\n",
      "Training Time (in minutes) = 2.4908801237742106\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "images, labels = next(iter(valloader))\r\n",
    "#images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "img = 0.0357*images[0].view(1, 784)\r\n",
    "img_gpu = img.to(device)\r\n",
    "with torch.no_grad():\r\n",
    "    out = model(img_gpu)\r\n",
    "\r\n",
    "ps = out.cpu()\r\n",
    "probab = list(ps.numpy()[0])\r\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Digit = 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "correct_count, all_count = 0, 0\r\n",
    "for images,labels in valloader:\r\n",
    "  images, labels = images.to(device), labels.to(device)\r\n",
    "  for i in range(len(labels)):\r\n",
    "    img = 0.0357*images[i].view(1, 784)\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(img)\r\n",
    "\r\n",
    "    \r\n",
    "    ps = out.cpu()\r\n",
    "    probab = list(ps.numpy()[0])\r\n",
    "    pred_label = probab.index(max(probab))\r\n",
    "    true_label = labels.cpu().numpy()[i]\r\n",
    "    if(true_label == pred_label):\r\n",
    "      correct_count += 1\r\n",
    "    all_count += 1\r\n",
    "\r\n",
    "print(\"Number Of Images Tested =\", all_count)\r\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9046\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}