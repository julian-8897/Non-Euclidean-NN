{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from models import vae_HYP\r\n",
    "import geoopt\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from hypmath import poincareball"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18304/3450961491.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvae_HYP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeoopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transform = transforms.Compose([\r\n",
    "        transforms.ToTensor(),\r\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\r\n",
    "        ])\r\n",
    "\r\n",
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET',\r\n",
    "                              download=True, train=True, transform=transform)\r\n",
    "testset = datasets.MNIST('PATH_TO_STORE_TESTSET',\r\n",
    "                            download=True, train=False, transform=transform)\r\n",
    "\r\n",
    "size = len(trainset)\r\n",
    "print(size)\r\n",
    "\r\n",
    "#Splitting training set into training and validation data\r\n",
    "train_data, val_data = torch.utils.data.random_split(trainset, [int(size-size*0.2), int(size*0.2)])\r\n",
    "\r\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True, num_workers=6, pin_memory=True)\r\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=256, num_workers=6, pin_memory=True)\r\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=True, num_workers=6, pin_memory=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\pc 3\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = vae_HYP.VariationalAutoencoder(latent_dims=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "learning_rate = 1e-3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optim = geoopt.optim.RiemannianAdam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Training function\r\n",
    "def train_epoch(vae, dataloader, optimizer):\r\n",
    "    # Set train mode for both the encoder and the decoder\r\n",
    "    vae.train()\r\n",
    "    train_loss = 0.0\r\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\r\n",
    "    for x, _ in dataloader: \r\n",
    "        # Move tensor to the proper device\r\n",
    "        # x = x.to(device)\r\n",
    "        x_hat = vae(x)\r\n",
    "        # Evaluate loss\r\n",
    "        #ball = poincareball.PoincareBall()\r\n",
    "        loss = ((x - x_hat)**2).sum() + (vae.encoder.kl)\r\n",
    "\r\n",
    "        # Backward pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        # Print batch loss\r\n",
    "        print('\\t partial train loss (single batch): %f' % (loss.item()))\r\n",
    "        train_loss+=loss.item()\r\n",
    "\r\n",
    "    return train_loss / len(dataloader.dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Testing function\r\n",
    "def test_epoch(vae, dataloader):\r\n",
    "    # Set evaluation mode for encoder and decoder\r\n",
    "    vae.eval()\r\n",
    "    val_loss = 0.0\r\n",
    "    with torch.no_grad(): # No need to track the gradients\r\n",
    "        for x, _ in dataloader:\r\n",
    "            # Move tensor to the proper device\r\n",
    "            # x = x.to(device)\r\n",
    "            # Encode data\r\n",
    "            encoded_data = vae.encoder(x)\r\n",
    "            # Decode data\r\n",
    "            x_hat = vae(x)\r\n",
    "            loss = ((x - x_hat)**2).sum() + vae.encoder.kl\r\n",
    "            val_loss += loss.item()\r\n",
    "\r\n",
    "    return val_loss / len(dataloader.dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_ae_outputs(encoder,decoder,n):\r\n",
    "    plt.figure(figsize=(10,4.5))\r\n",
    "    for i in range(n):\r\n",
    "      ax = plt.subplot(2,n,i+1)\r\n",
    "      img = testset[i][0].unsqueeze(0)\r\n",
    "      encoder.eval()\r\n",
    "      decoder.eval()\r\n",
    "      with torch.no_grad():\r\n",
    "         rec_img  = decoder(encoder(img))\r\n",
    "      plt.imshow(img.squeeze().numpy(), cmap='gist_gray')\r\n",
    "      ax.get_xaxis().set_visible(False)\r\n",
    "      ax.get_yaxis().set_visible(False)  \r\n",
    "      if i == n//2:\r\n",
    "        ax.set_title('Original images')\r\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\r\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \r\n",
    "      ax.get_xaxis().set_visible(False)\r\n",
    "      ax.get_yaxis().set_visible(False)  \r\n",
    "      if i == n//2:\r\n",
    "         ax.set_title('Reconstructed images')\r\n",
    "    plt.show()   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 10\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "   train_loss = train_epoch(model, trainloader,optim)\r\n",
    "   val_loss = test_epoch(model, valloader)\r\n",
    "   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\r\n",
    "   plot_ae_outputs(model.encoder, model.decoder,n=6)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t partial train loss (single batch): 68026.820312\n",
      "\t partial train loss (single batch): 67112.125000\n",
      "\t partial train loss (single batch): 65427.019531\n",
      "\t partial train loss (single batch): 63954.089844\n",
      "\t partial train loss (single batch): 62428.304688\n",
      "\t partial train loss (single batch): 61330.921875\n",
      "\t partial train loss (single batch): 60198.007812\n",
      "\t partial train loss (single batch): 59383.863281\n",
      "\t partial train loss (single batch): 58683.015625\n",
      "\t partial train loss (single batch): 57918.867188\n",
      "\t partial train loss (single batch): 57533.007812\n",
      "\t partial train loss (single batch): 57061.835938\n",
      "\t partial train loss (single batch): 56460.125000\n",
      "\t partial train loss (single batch): 56164.218750\n",
      "\t partial train loss (single batch): 55773.738281\n",
      "\t partial train loss (single batch): 55694.031250\n",
      "\t partial train loss (single batch): 55333.910156\n",
      "\t partial train loss (single batch): 54741.867188\n",
      "\t partial train loss (single batch): 54457.320312\n",
      "\t partial train loss (single batch): 53970.320312\n",
      "\t partial train loss (single batch): 53703.492188\n",
      "\t partial train loss (single batch): 53407.042969\n",
      "\t partial train loss (single batch): 52881.441406\n",
      "\t partial train loss (single batch): 52484.500000\n",
      "\t partial train loss (single batch): 52367.593750\n",
      "\t partial train loss (single batch): 51993.617188\n",
      "\t partial train loss (single batch): 51751.859375\n",
      "\t partial train loss (single batch): 51481.113281\n",
      "\t partial train loss (single batch): 51196.667969\n",
      "\t partial train loss (single batch): 50896.347656\n",
      "\t partial train loss (single batch): 50741.898438\n",
      "\t partial train loss (single batch): 50542.117188\n",
      "\t partial train loss (single batch): 50206.863281\n",
      "\t partial train loss (single batch): 50088.804688\n",
      "\t partial train loss (single batch): 49727.878906\n",
      "\t partial train loss (single batch): 49480.531250\n",
      "\t partial train loss (single batch): 49277.847656\n",
      "\t partial train loss (single batch): 49090.683594\n",
      "\t partial train loss (single batch): 48727.687500\n",
      "\t partial train loss (single batch): 48407.265625\n",
      "\t partial train loss (single batch): 48222.167969\n",
      "\t partial train loss (single batch): 48148.457031\n",
      "\t partial train loss (single batch): 47909.640625\n",
      "\t partial train loss (single batch): 47621.640625\n",
      "\t partial train loss (single batch): 47408.105469\n",
      "\t partial train loss (single batch): 47176.011719\n",
      "\t partial train loss (single batch): 46916.398438\n",
      "\t partial train loss (single batch): 46678.808594\n",
      "\t partial train loss (single batch): 46457.761719\n",
      "\t partial train loss (single batch): 46343.718750\n",
      "\t partial train loss (single batch): 46081.648438\n",
      "\t partial train loss (single batch): 45836.246094\n",
      "\t partial train loss (single batch): 45623.308594\n",
      "\t partial train loss (single batch): 45437.632812\n",
      "\t partial train loss (single batch): 45189.964844\n",
      "\t partial train loss (single batch): 44954.929688\n",
      "\t partial train loss (single batch): 44784.097656\n",
      "\t partial train loss (single batch): 44659.996094\n",
      "\t partial train loss (single batch): 44460.671875\n",
      "\t partial train loss (single batch): 44155.234375\n",
      "\t partial train loss (single batch): 43912.187500\n",
      "\t partial train loss (single batch): 43789.390625\n",
      "\t partial train loss (single batch): 43641.421875\n",
      "\t partial train loss (single batch): 43403.894531\n",
      "\t partial train loss (single batch): 43124.757812\n",
      "\t partial train loss (single batch): 43021.042969\n",
      "\t partial train loss (single batch): 42760.089844\n",
      "\t partial train loss (single batch): 42513.542969\n",
      "\t partial train loss (single batch): 42540.648438\n",
      "\t partial train loss (single batch): 42085.449219\n",
      "\t partial train loss (single batch): 41959.867188\n",
      "\t partial train loss (single batch): 41758.671875\n",
      "\t partial train loss (single batch): 41513.609375\n",
      "\t partial train loss (single batch): 41263.566406\n",
      "\t partial train loss (single batch): 41131.730469\n",
      "\t partial train loss (single batch): 40794.160156\n",
      "\t partial train loss (single batch): 40605.398438\n",
      "\t partial train loss (single batch): 40444.476562\n",
      "\t partial train loss (single batch): 40185.792969\n",
      "\t partial train loss (single batch): 40085.738281\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18088/3844939306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m    \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18088/1578143464.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(vae, dataloader, optimizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Print batch loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}