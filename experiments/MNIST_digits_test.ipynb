{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2454a3adb90052121e3433f22c2e288f84a7f03217a2a46086941be12932708b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from models import ff_eucl, ff_hyp\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import geoopt\r\n",
    "from time import time\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "import helper\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#CUDA check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print('Using {}'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                                #transforms.Normalize((0.1307,), (0.3081,)), \r\n",
    "                              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training, validation and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_set = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\r\n",
    "test_set = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\r\n",
    "\r\n",
    "size = len(train_set)\r\n",
    "print(size)\r\n",
    "\r\n",
    "train_data, val_data = torch.utils.data.random_split(train_set, [int(size-size*0.2), int(size*0.2)])\r\n",
    "\r\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=6, pin_memory=True)\r\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle= True, num_workers=6, pin_memory=True)\r\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle= True, num_workers=6, pin_memory=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\pc 3\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "# ball = geoopt.PoincareBall()\r\n",
    "# images, labels = next(iter(trainloader))\r\n",
    "# images = images.view(images.shape[0], -1)\r\n",
    "# grid = torchvision.utils.make_grid(images)\r\n",
    "# tb = SummaryWriter()\r\n",
    "# tb.add_image(\"images\", grid)\r\n",
    "# tb.add_graph(model, images)\r\n",
    "# model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from itertools import product\r\n",
    "parameters = dict(\r\n",
    "    lr = [0.1, 0.01, 0.001],\r\n",
    "    batch_size = [64,128,256],\r\n",
    "    shuffle = [True, False]\r\n",
    ")\r\n",
    "\r\n",
    "param_values = [v for v in parameters.values()]\r\n",
    "print(param_values)\r\n",
    "\r\n",
    "for lr,batch_size, shuffle in product(*param_values):\r\n",
    "    print(lr, batch_size, shuffle)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.1, 0.01, 0.001], [64, 128, 256], [True, False]]\n",
      "0.1 64 True\n",
      "0.1 64 False\n",
      "0.1 128 True\n",
      "0.1 128 False\n",
      "0.1 256 True\n",
      "0.1 256 False\n",
      "0.01 64 True\n",
      "0.01 64 False\n",
      "0.01 128 True\n",
      "0.01 128 False\n",
      "0.01 256 True\n",
      "0.01 256 False\n",
      "0.001 64 True\n",
      "0.001 64 False\n",
      "0.001 128 True\n",
      "0.001 128 False\n",
      "0.001 256 True\n",
      "0.001 256 False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#criterion = nn.CrossEntropyLoss()\r\n",
    "# #criterion = nn.NLLLoss()\r\n",
    "# images, labels = next(iter(trainloader))\r\n",
    "# images, labels = images.to(device), labels.to(device)\r\n",
    "# images = 0.0357*images.view(images.shape[0], -1)\r\n",
    "# print(images)\r\n",
    "\r\n",
    "# out = model(images) #output\r\n",
    "# print(out)\r\n",
    "# loss = criterion(out, labels) #calculate the loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#optimizer = geoopt.optim.RiemannianSGD(model.parameters(), lr=learning_rate, momentum=momentum)\r\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training, validating, prediction, and hyperparameter tuning functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "### Single prediction function\r\n",
    "def get_num_correct(preds, labels):\r\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\r\n",
    "\r\n",
    "### Training function\r\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\r\n",
    "    model.train()\r\n",
    "    train_loss = 0\r\n",
    "    total_correct = 0\r\n",
    "    for images, labels in dataloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        # Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        train_loss += loss.item()\r\n",
    "        total_correct += get_num_correct(output, labels)\r\n",
    "        #backpropagation\r\n",
    "        loss.backward()      \r\n",
    "        #Weight optimization\r\n",
    "        optimizer.step()  \r\n",
    "\r\n",
    "    return train_loss, total_correct\r\n",
    "\r\n",
    "### Validation function\r\n",
    "def val_epoch(model, dataloader, criterion):\r\n",
    "    model.eval()\r\n",
    "    val_loss = 0\r\n",
    "    val_correct = 0\r\n",
    "    for  images, labels in dataloader:\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "        #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)  \r\n",
    "        val_loss += loss.item()\r\n",
    "        val_correct += get_num_correct(output, labels)\r\n",
    "    \r\n",
    "    return val_loss, val_correct\r\n",
    "\r\n",
    "\r\n",
    "### Hyperparameter tuning function\r\n",
    "def hparams_tune(epochs):\r\n",
    "    for run_id, (lr,batch_size, shuffle) in enumerate(product(*param_values)):\r\n",
    "        print(\"run id:\", run_id + 1)\r\n",
    "        model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "        trainloader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, shuffle = shuffle)\r\n",
    "        valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle= shuffle)\r\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "        criterion = torch.nn.CrossEntropyLoss()\r\n",
    "        comment = f' batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\r\n",
    "        tb = SummaryWriter(comment=comment)\r\n",
    "        \r\n",
    "        for epoch in range(epochs):\r\n",
    "            train_loss, total_correct = train_epoch(model, trainloader, optimizer, criterion)\r\n",
    "            val_loss, val_correct = val_epoch(model, valloader, criterion)\r\n",
    "                \r\n",
    "            tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "            tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "            tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "            tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "\r\n",
    "            print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "            \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "        \r\n",
    "        tb.add_hparams(\r\n",
    "                {\"lr\": lr, \"bsize\": batch_size, \"shuffle\":shuffle},\r\n",
    "                {\r\n",
    "                    \"training accuracy\": total_correct/ len(train_data),\r\n",
    "                    \"validation accuracy\": val_correct/ len(val_data),\r\n",
    "                    \"training loss\": train_loss,\r\n",
    "                    \"validation loss\": val_loss,\r\n",
    "                },\r\n",
    "            )\r\n",
    "    tb.close()\r\n",
    "\r\n",
    "    return None\r\n",
    "\r\n",
    "### Model evaluation\r\n",
    "def model_eval(model, epochs, trainloader, valloader, optimizer, criterion):\r\n",
    "    tb = SummaryWriter()\r\n",
    "    for epoch in range(epochs):\r\n",
    "        train_loss, total_correct = train_epoch(model, trainloader, optimizer, criterion) \r\n",
    "        val_loss, val_correct = val_epoch(model, valloader, criterion)\r\n",
    "        tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "        tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "        tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "        tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "        print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "        \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=1e-1)\r\n",
    "epochs=10\r\n",
    "#Hyperparameter tuning\r\n",
    "#hparams_tune(epochs)\r\n",
    "#Model evaluation\r\n",
    "criterion = torch.nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n",
    "model_eval(model, epochs, trainloader, valloader, optimizer, criterion)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12100/1122870763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12100/2432879672.py\u001b[0m in \u001b[0;36mmodel_eval\u001b[1;34m(model, epochs, trainloader, valloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mtb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12100/2432879672.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Flatten MNIST images into a 784 long vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for run_id, (lr,batch_size, shuffle) in enumerate(product(*param_values)):\r\n",
    "#     print(\"run id:\", run_id + 1)\r\n",
    "#     model = ff_eucl.EuclFF(784, 512, 256, 10, nn.ReLU())\r\n",
    "#     trainloader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, shuffle = shuffle)\r\n",
    "#     valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle= shuffle)\r\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\r\n",
    "#     comment = f' batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\r\n",
    "#     tb = SummaryWriter(comment=comment)\r\n",
    "# # time0 = time()\r\n",
    "#     epochs = 10\r\n",
    "#     for epoch in range(epochs):\r\n",
    "#     #     model.train()\r\n",
    "#     #     train_loss = 0\r\n",
    "#     #     total_correct = 0\r\n",
    "#     #     for images, labels in trainloader:\r\n",
    "#     #         images, labels = images.to(device), labels.to(device)\r\n",
    "#     #         # Flatten MNIST images into a 784 long vector\r\n",
    "#     #         images = images.view(images.shape[0], -1)\r\n",
    "#     #         #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "#     #         # Training pass\r\n",
    "#     #         optimizer.zero_grad()\r\n",
    "#     #         output = model(images)\r\n",
    "#     #         loss = criterion(output, labels)  \r\n",
    "#     #         train_loss += loss.item()\r\n",
    "#     #         total_correct += get_num_correct(output, labels)\r\n",
    "#     #         #backpropagation\r\n",
    "#     #         loss.backward()      \r\n",
    "#     #         #Weight optimization\r\n",
    "#     #         optimizer.step()  \r\n",
    "#         train_loss, total_correct = train_epoch(model, trainloader, optimizer)\r\n",
    "#         val_loss, val_correct = val_epoch(model, valloader)\r\n",
    "#         # val_loss = 0\r\n",
    "#         # val_correct = 0\r\n",
    "#         # model.eval()\r\n",
    "#         # for  images, labels in valloader:\r\n",
    "#         #     images, labels = images.to(device), labels.to(device)\r\n",
    "#         #     # Flatten MNIST images into a 784 long vector\r\n",
    "#         #     images = images.view(images.shape[0], -1)\r\n",
    "#         #     #images = ball.projx(images.view(images.shape[0], -1))\r\n",
    "#         #     output = model(images)\r\n",
    "#         #     loss = criterion(output, labels)  \r\n",
    "#         #     val_loss += loss.item()\r\n",
    "#         #     val_correct += get_num_correct(output, labels)\r\n",
    "            \r\n",
    "#         tb.add_scalar(\"Training Loss\", train_loss, epoch)\r\n",
    "#         tb.add_scalar(\"Validation Loss\", val_loss, epoch)\r\n",
    "#         tb.add_scalar(\"Training Accuracy\", total_correct/len(train_data), epoch)\r\n",
    "#         tb.add_scalar(\"Validation Accuracy\", val_correct/len(val_data), epoch)\r\n",
    "\r\n",
    "#         print(\"epoch:\", epoch, \"training loss:\",train_loss, \"validation loss:\", val_loss,\r\n",
    "#         \"training accuracy:\", total_correct/len(train_data), \"validation accuracy:\", val_correct/len(val_data))\r\n",
    "    \r\n",
    "#     tb.add_hparams(\r\n",
    "#             {\"lr\": lr, \"bsize\": batch_size, \"shuffle\":shuffle},\r\n",
    "#             {\r\n",
    "#                 \"training accuracy\": total_correct/ len(train_data),\r\n",
    "#                 \"validation accuracy\": val_correct/ len(val_data),\r\n",
    "#                 \"training loss\": train_loss,\r\n",
    "#                 \"validation loss\": val_loss,\r\n",
    "#             },\r\n",
    "#         )\r\n",
    "# tb.close()\r\n",
    "# #     else:\r\n",
    "# #         print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
    "# # print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# images, labels = next(iter(testloader))\r\n",
    "# #images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "# img = images[0].view(1, 784)\r\n",
    "# #img = ball.projx(images[0].view(1, 784))\r\n",
    "# #img_gpu = img.to(device)\r\n",
    "# with torch.no_grad():\r\n",
    "#     out = nn.LogSoftmax(model(img))\r\n",
    "\r\n",
    "# ps = out.cpu()\r\n",
    "# print(ps)\r\n",
    "# probab = list(ps.numpy()[0])\r\n",
    "# print(probab)\r\n",
    "# print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Model Prediction and Model Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# correct_count, all_count = 0, 0\r\n",
    "# for images,labels in testloader:\r\n",
    "#   images, labels = images.to(device), labels.to(device)\r\n",
    "#   for i in range(len(labels)):\r\n",
    "#     img = images[i].view(1, 784)\r\n",
    "#     #img = ball.projx(images[i].view(1, 784))\r\n",
    "#     with torch.no_grad():\r\n",
    "#         out = model(img)\r\n",
    "\r\n",
    "#     ps = out.cpu()\r\n",
    "#     probab = list(ps.numpy()[0])\r\n",
    "#     pred_label = probab.index(max(probab))\r\n",
    "#     true_label = labels.cpu().numpy()[i]\r\n",
    "#     if(true_label == pred_label):\r\n",
    "#       correct_count += 1\r\n",
    "#     all_count += 1\r\n",
    "    \r\n",
    "# print(\"Number Of Images Tested =\", all_count)\r\n",
    "# print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current status of experiments:\r\n",
    "1. Using just Hyperboic Linear modules, and with the appropriate self-tuned hyperparameters, and a batch size of 512, the average accuracy was around 90 percent\r\n",
    "2. With the use of activation functions(ReLu, ReLu, then LogSoftMax at the output layer), (by applying the functions in the tangent space, then mapping it back to the hyperbolic space), we see an increase in the model accuracy to about 97-98 percent.\r\n",
    "3. To account for the correct class probabilities , linear layer was used as the output layer instead, together with the crossentropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}